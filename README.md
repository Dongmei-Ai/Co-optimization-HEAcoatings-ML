Co-optimization of hardness and toughness of high-entropy alloy coatings driven by machine learning strategies

Backgrounds：
High-entropy alloy (HEA) coatings find significant applications across various engineering fields, including energy, power, mechanical manufacturing, and biomedicine, due to the exceptional mechanical and physical properties. However, challenges such as small-sample datasets imbalances and the inherent trade-offs between hardness and toughness have hindered the accelerated design of materials when optimizing HEA coatings using machine learning algorithms. This study proposes a machine learning strategy that employs de-heterogeneous conditional generative adversarial networks (DHG-CGAN) to mitigate the heterogeneity issue. This strategy improves the accuracy of machine learning models while facilitating the discovery of new systems and exploring the intrinsic relationships between material descriptors, processes, and compositions. Experimental results demonstrated that the DHG-CGAN outperformed existing generative models in producing high-quality data on HEA coatings. The implementation of this machine learning strategy significantly improved model performance, with R² values reaching 0.93 and 0.88 for hardness and elastic modulus, respectively. Moreover, the study identified potential HEA systems with superior overall performance within the composition space and determined optimal process parameters and compositional ratios for these systems. Finally, model explanability analysis revealed that W and Zr influenced hardness and toughness by affecting the variance of evaporation heat and the atomic energy of the coating. This strategy offers a novel approach to accelerating the design of HEA coatings.

Optimize the process:
(a)Feature selection→(b)Data augmentation→(c)Training and saving prediction model →(d)multi-objective optimization→(e)Interpretability analysis.

(a) Execute Featuresselect.py to obtain the feature selection results.
(b) Run CGAN and DHG-CGAN comparison.py to compare the performance of CGAN and DHG-CGAN.
(c) Execute Fitting results of the hardness and elastic modulus datasets.py to analyze the model's fitting performance after data enhancement and evaluate its predictive ability on the test set.
(d) Run Multi-objective optimization results.py to obtain the multi-objective optimization results.
(e) Execute The SHAP summary plot.py to generate the SHAP summary plot for the prediction model. Additionally, run Effect of descriptors and composition on the properties of HEA coatings.py to explore the relationship between descriptors and material properties in the virtual dataset.

Note
1.The virtual data generated by DHG-CGAN are saved in Augmented_dataset_H and Augmented_dataset_M. The pre-trained CGAN and DHG-CGAN generators are stored in H_cgan.keras, H_dhgcgan.keras, M_cgan.keras, and M_dhgcgan.keras. Running GAN model train.py will retrain the CGAN and DHG-CGAN models and generate new virtual data.
2.The enhanced models are saved as ML-model_H.pkl and ML-model_M.pkl. The prediction results for the models on the training set, test set, and generated data are saved in Prediction_results_H and Prediction_results_M.
3.found_system5.csv, found_system6.csv, and found_system7.csv correspond to the traversed component spaces for five, six, and seven elements, respectively.
4.found_pareto contains the optimal results obtained by the NSGA-II algorithm from searching in found_system5.csv, found_system6.csv, and found_system7.csv. NSGA2.py is used to implement the multi-objective optimization process.
5.data.csv is a sample containing both hardness and modulus data within the dataset(References for the dataset are listed in “Dataset reference.docx").
